version: '2.1'

services:
  # this is our kafka cluster.
  kafka-cluster:
    image: landoop/fast-data-dev:latest
    container_name: kafka
    environment:
      ADV_HOST: 127.0.0.1
      RUNTESTS: 0                 # Disable Running tests so the cluster starts faster
    ports:
      - 2181:2181                 # Zookeeper
      - 3030:3030                 # Landoop UI
      - 8081-8083:8081-8083       # REST Proxy, Schema Registry, Kafka Connect ports
      - 9581-9585:9581-9585       # JMX Ports
      - 9092:9092                 # Kafka Broker
    volumes:
      - ./kafka-connect-jdbc:/usr/share/java/kafka-connect-jdbc # bind mount for Kafka additional JDBC drivers

  # This configuration allows you to start Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.1.1
    container_name: elasticsearch
    environment:
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - 9200:9200
      - 9300:9300

  # This configuration allows you to start Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:6.1.1
    container_name: kibana
    ports:
      - 5601:5601
    depends_on:
      - elasticsearch

  # This configuration allows you to start Postgres
  postgres:
    image: postgres:9.6
    container_name: postgres
    environment:
      POSTGRES_USER: postgres     # define credentials
      POSTGRES_PASSWORD: postgres # define credentials
      POSTGRES_DB: postgres       # define database
    volumes:
      - ./postgres_users:/docker-entrypoint-initdb.d # create additional users
    ports:
      - 5432:5432                 # Postgres port

  # This configuration allows you to start MS SQL DB
  mssql:
    image: microsoft/mssql-server-linux:latest
    environment:
      ACCEPT_EULA: Y
      SA_PASSWORD: P4sw0rd123
    ports:
      - 1433:1433                 # MSSQL port

  # This configuration allows you to start My SQL DB
  mysql:
    image: mysql
    container_name: mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: mysql  # password for user root
    ports:
      - 3306:3306

  # This configuration allows you to start Redis DB
  redis:
    image: 'redis:3.2.7'
    container_name: redis

  # This configuration allows you to start airflow webserver
  webserver:
    image: puckel/docker-airflow:1.9.0
    restart: always
    depends_on:
        - postgres
        - redis
    environment:
        - LOAD_EX=n
        - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
        - EXECUTOR=Celery
    volumes:
        - ./airflow_dags:/usr/local/airflow/dags
        - ./airflow_plugins:/usr/local/airflow/plugins
    ports:
        - "8080:8080"
    command: airflow_webserver
    healthcheck:
        test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
        interval: 30s
        timeout: 30s
        retries: 3

  # This configuration allows you to start airflow flower for Celery mode
  flower:
    image: puckel/docker-airflow:1.9.0
    restart: always
    depends_on:
        - redis
    environment:
        - EXECUTOR=Celery
    ports:
        - "5555:5555"
    command: flower

  # This configuration allows you to start airflow scheduler
  scheduler:
    image: puckel/docker-airflow:1.9.0
    restart: always
    depends_on:
        - webserver
    volumes:
        - ./airflow_dags:/usr/local/airflow/dags
        - ./airflow_plugins:/usr/local/airflow/plugins
    environment:
        - LOAD_EX=n
        - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
        - EXECUTOR=Celery
    command: scheduler

  # This configuration allows you to start airflow worker
  worker:
    image: puckel/docker-airflow:1.9.0
    restart: always
    depends_on:
        - scheduler
    volumes:
        - ./airflow_dags:/usr/local/airflow/dags
        - ./airflow_plugins:/usr/local/airflow/plugins
    environment:
        - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
        - EXECUTOR=Celery
    command: worker

  # This configuration allows you to start Hadoop name node
  namenode:
    image: uhopper/hadoop-namenode:2.8.1
    container_name: namenode
    environment:
      - CLUSTER_NAME=hadoop_dev
      - HDFS_CONF_dfs_replication=1 # change replication to 1, because only 1 data node
      - HDFS_CONF_dfs_permissions_enabled=false # permission checking is turned off
    ports:
      - 8020:8020
      - 50070:50070

  # This configuration allows you to start Hadoop data node
  datanode1:
    image: uhopper/hadoop-datanode:2.8.1
    container_name: datanode1
    hostname: datanode1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - 50010:50010
      - 50020:50020
      - 50075:50075

  # This configuration allows you to start YARN resource manager
  resourcemanager:
    image: uhopper/hadoop-resourcemanager:2.8.1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_log___aggregation___enable=true
    ports:
      - 8088:8088

  # This configuration allows you to start YARN node manager
  nodemanager1:
    image: uhopper/hadoop-nodemanager:2.8.1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - YARN_CONF_yarn_log___aggregation___enable=true
      - YARN_CONF_yarn_nodemanager_remote___app___log___dir=/app-logs

  # This configuration allows you to start Hadoop edge node with Spark
  spark:
    image: uhopper/hadoop-spark:2.1.2_2.8.1
    container_name: spark
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
    command: tail -f /var/log/dmesg
    ports:
      - 18080:18080
      - 4040:4040
    volumes:
      - ./spark-jobs:/opt/transformations     # bind mount for Spark jar jobs

  # This configuration allows you to start local S3 based on fakes3 (https://github.com/jubos/fake-s3)
  s3:
    image: lphoward/fake-s3
    ports:
      - 4569:4569

  dynamodb:
    image: cnadiminti/dynamodb-local
    container_name: dynamodb
    ports:
      - 8000:8000
